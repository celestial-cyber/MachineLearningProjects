# ---------------------------------------
# Decision Tree (ID3 Algorithm) Example
# ---------------------------------------

import math
import pandas as pd
from operator import itemgetter

# ---------------------------------------
# Embedded Dataset
# ---------------------------------------
# Outlook, Temperature, Humidity, Wind Speed, Play
data = pd.DataFrame([
    ['Sunny', 'Hot', 'High', 'Weak', 'No'],
    ['Sunny', 'Hot', 'High', 'Strong', 'No'],
    ['Overcast', 'Hot', 'High', 'Weak', 'Yes'],
    ['Rainy', 'Mild', 'High', 'Weak', 'Yes'],
    ['Rainy', 'Cool', 'Normal', 'Weak', 'Yes'],
    ['Rainy', 'Cool', 'Normal', 'Strong', 'No'],
    ['Overcast', 'Cool', 'Normal', 'Strong', 'Yes'],
    ['Sunny', 'Mild', 'High', 'Weak', 'No'],
    ['Sunny', 'Cool', 'Normal', 'Weak', 'Yes'],
    ['Rainy', 'Mild', 'Normal', 'Weak', 'Yes'],
    ['Sunny', 'Mild', 'Normal', 'Strong', 'Yes'],
    ['Overcast', 'Mild', 'High', 'Strong', 'Yes'],
    ['Overcast', 'Hot', 'Normal', 'Weak', 'Yes'],
    ['Rainy', 'Mild', 'High', 'Strong', 'No']
], columns=['Outlook', 'Temperature', 'Humidity', 'WindSpeed', 'Play'])

print("\n--- Training Data ---\n")
print(data)

# ---------------------------------------
# Decision Tree Class
# ---------------------------------------
class DecisionTree:
    def __init__(self, df, target, positive, parent_val, parent):
        self.data = df
        self.target = target
        self.positive = positive
        self.parent_val = parent_val
        self.parent = parent
        self.childs = []
        self.decision = ''

    # Compute entropy
    def _get_entropy(self, data):
        p = sum(data[self.target] == self.positive)
        n = data.shape[0] - p
        if p == 0 or n == 0:
            return 0
        p_ratio = p / (p + n)
        n_ratio = 1 - p_ratio
        return -(p_ratio * math.log2(p_ratio) + n_ratio * math.log2(n_ratio))

    # Compute information gain for a feature
    def _get_gain(self, feat):
        total_entropy = self._get_entropy(self.data)
        weighted_entropy = 0
        for val in self.data[feat].unique():
            subset = self.data[self.data[feat] == val]
            weighted_entropy += (len(subset) / len(self.data)) * self._get_entropy(subset)
        return total_entropy - weighted_entropy

    # Find best feature to split
    def _get_splitter(self):
        self.splitter = max(self.gains, key=itemgetter(1))[0]

    # Build tree recursively
    def update_nodes(self):
        self.features = [col for col in self.data.columns if col != self.target]
        self.entropy = self._get_entropy(self.data)

        if self.entropy != 0:
            # Compute gains for all features
            self.gains = [(feat, self._get_gain(feat)) for feat in self.features]
            self._get_splitter()

            # Split data
            residual_columns = [col for col in self.data.columns if col != self.splitter]
            for val in self.data[self.splitter].unique():
                subset = self.data[self.data[self.splitter] == val][residual_columns]
                child_node = DecisionTree(subset, self.target, self.positive, val, self.splitter)
                child_node.update_nodes()
                self.childs.append(child_node)

    # Tree printer
    def print_tree(self, indent=""):
        for child in self.childs:
            print(f"{indent}{self.parent} = {child.parent_val} â†’ ", end="")
            if child.entropy == 0:
                # Pure node
                decision_class = child.data[self.target].iloc[0]
                print(f"[Leaf: {decision_class}]")
            else:
                print()
                child.print_tree(indent + "  ")

# ---------------------------------------
# Run the Algorithm
# ---------------------------------------
dt = DecisionTree(data, target='Play', positive='Yes', parent_val='', parent='')
dt.update_nodes()

print("\n--- Decision Tree ---\n")
dt.print_tree()
